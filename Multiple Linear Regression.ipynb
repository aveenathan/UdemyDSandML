{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Import the Libraries:\n",
    "\n",
    "1. Numpy - Math functions\n",
    "2. MatplotLib - Visualization\n",
    "3. SciKitLearn \n",
    "\n",
    "    a) Build Models \n",
    "    b) Preprocessing (Encoding,One-Hot Encoding)\n",
    "\n",
    "Pandas Package \n",
    "    \n",
    "    a) Read Files into Dataframes \n",
    "    b) Create, Manipulate Dataframes \n",
    "    c) Create Feature Matrix \n",
    "    d) Create Output Vector\n",
    "\n",
    "SciKit Learn Details:\n",
    "\n",
    "Preprocess Data \n",
    "\n",
    "    a) Imputer - Missing values using strategies like Mean \n",
    "    b) Convert Categorical data to Numbers - LabelEncoder \n",
    "    c) Convert Numbers into One Hot Encoding - OneHotEncoder \n",
    "    d) Model Selection - Split into Training and Test Set Data \n",
    "    e) Scaling Data - Standaradization / Normalization\n",
    "\n",
    "Build Linear Regression Models \n",
    "\n",
    "    a) linear_model library, LinearModel object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing for Multiple Linear Regression Model\n",
    "\n",
    "\n",
    "The key item to notice in this is the Dummy Variable Trap\n",
    "When there are n levels of dumy variables created, we will need to remove one vector. \n",
    "For example in this example, state is encoded as follows\n",
    "New York, Florida, California. When we encode this using one-hot encoding, When we have the value of the two states, we can infer the third - there is Collienearity and values of the other dummy variables determines the last dummy variable. So one has to be removed. \n",
    "\n",
    "# Handle Dummy Variable Trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaa/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/aaa/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "#-----------------Preprocessing Section -------------------------------------\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 27 21:24:44 2019\n",
    "Multiple Linear Regression\n",
    "@author: Anand\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Removing first column to avoid the Dummy Variable Trap\n",
    "X= X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 0)\n",
    "\n",
    "#---------------End of Pre-processing Section ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model (to Training set ) and Predicting (on Test set) \n",
    "\n",
    "Use the LinearRegression Object from the linear_model library\n",
    "Use the Fit method of the Regressor object to the training Set\n",
    "Use the Predict methd of the Regressor object on the Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Linear Regression Model to the Taining Set - Ordinary least squares Linear Regression.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "# Predcting the test set observations\n",
    "y_pred=regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Optimal Model usinf Back Validation\n",
    "Create a linear regrssion model using the Ordinary Least Squares (OLS) with all the independant variables.\n",
    "Set a Significance level say 5% (SL = .05)\n",
    "Check the P value of the Model for each of the independent variables\n",
    "Based upon this P value and Significance Level decide to keep/remove the Variables with high P Values. \n",
    "Successively Do this and inspect the Model using the Summary function.\n",
    "Use a Line to predict the training inputs, and predicted outputs on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an optimal model using back validation\n",
    "# As the stats model does not take care of intercept, we need to incldude a ones array\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt=X[:, [0,1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   205.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 31 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>2.90e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:34:04</td>     <th>  Log-Likelihood:    </th> <td> -526.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1064.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1073.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  2.73e+04</td> <td> 3185.530</td> <td>    8.571</td> <td> 0.000</td> <td> 2.09e+04</td> <td> 3.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  2.73e+04</td> <td> 3185.530</td> <td>    8.571</td> <td> 0.000</td> <td> 2.09e+04</td> <td> 3.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1091.1075</td> <td> 3377.087</td> <td>    0.323</td> <td> 0.748</td> <td>-5710.695</td> <td> 7892.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -39.3434</td> <td> 3309.047</td> <td>   -0.012</td> <td> 0.991</td> <td>-6704.106</td> <td> 6625.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.8609</td> <td>    0.031</td> <td>   27.665</td> <td> 0.000</td> <td>    0.798</td> <td>    0.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0527</td> <td>    0.050</td> <td>   -1.045</td> <td> 0.301</td> <td>   -0.154</td> <td>    0.049</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.275</td> <th>  Durbin-Watson:     </th> <td>   1.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  19.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.953</td> <th>  Prob(JB):          </th> <td>6.57e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.369</td> <th>  Cond. No.          </th> <td>7.08e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.15e-24. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.948\n",
       "Model:                            OLS   Adj. R-squared:                  0.943\n",
       "Method:                 Least Squares   F-statistic:                     205.0\n",
       "Date:                Sun, 31 Mar 2019   Prob (F-statistic):           2.90e-28\n",
       "Time:                        17:34:04   Log-Likelihood:                -526.75\n",
       "No. Observations:                  50   AIC:                             1064.\n",
       "Df Residuals:                      45   BIC:                             1073.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        2.73e+04   3185.530      8.571      0.000    2.09e+04    3.37e+04\n",
       "x1           2.73e+04   3185.530      8.571      0.000    2.09e+04    3.37e+04\n",
       "x2          1091.1075   3377.087      0.323      0.748   -5710.695    7892.910\n",
       "x3           -39.3434   3309.047     -0.012      0.991   -6704.106    6625.420\n",
       "x4             0.8609      0.031     27.665      0.000       0.798       0.924\n",
       "x5            -0.0527      0.050     -1.045      0.301      -0.154       0.049\n",
       "==============================================================================\n",
       "Omnibus:                       14.275   Durbin-Watson:                   1.197\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               19.260\n",
       "Skew:                          -0.953   Prob(JB):                     6.57e-05\n",
       "Kurtosis:                       5.369   Cond. No.                     7.08e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.15e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the P values above, we can remove x3 above\n",
    "x3 above corrsponds to the state variable of the data set, that will be removed as part of the back validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   278.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 31 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>1.68e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:47:32</td>     <th>  Log-Likelihood:    </th> <td> -526.81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1062.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1069.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 2.753e+04</td> <td> 3072.973</td> <td>    8.960</td> <td> 0.000</td> <td> 2.13e+04</td> <td> 3.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 2.753e+04</td> <td> 3072.973</td> <td>    8.960</td> <td> 0.000</td> <td> 2.13e+04</td> <td> 3.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> -573.7029</td> <td> 2838.043</td> <td>   -0.202</td> <td> 0.841</td> <td>-6286.386</td> <td> 5138.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8624</td> <td>    0.030</td> <td>   28.282</td> <td> 0.000</td> <td>    0.801</td> <td>    0.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0530</td> <td>    0.050</td> <td>   -1.063</td> <td> 0.294</td> <td>   -0.154</td> <td>    0.047</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.902</td> <th>  Durbin-Watson:     </th> <td>   1.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.964</td> <th>  Prob(JB):          </th> <td>2.48e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.543</td> <th>  Cond. No.          </th> <td>1.37e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 5.78e-23. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.948\n",
       "Model:                            OLS   Adj. R-squared:                  0.944\n",
       "Method:                 Least Squares   F-statistic:                     278.7\n",
       "Date:                Sun, 31 Mar 2019   Prob (F-statistic):           1.68e-29\n",
       "Time:                        16:47:32   Log-Likelihood:                -526.81\n",
       "No. Observations:                  50   AIC:                             1062.\n",
       "Df Residuals:                      46   BIC:                             1069.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       2.753e+04   3072.973      8.960      0.000    2.13e+04    3.37e+04\n",
       "x1          2.753e+04   3072.973      8.960      0.000    2.13e+04    3.37e+04\n",
       "x2          -573.7029   2838.043     -0.202      0.841   -6286.386    5138.981\n",
       "x3             0.8624      0.030     28.282      0.000       0.801       0.924\n",
       "x4            -0.0530      0.050     -1.063      0.294      -0.154       0.047\n",
       "==============================================================================\n",
       "Omnibus:                       14.902   Durbin-Watson:                   1.199\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.212\n",
       "Skew:                          -0.964   Prob(JB):                     2.48e-05\n",
       "Kurtosis:                       5.543   Cond. No.                     1.37e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.78e-23. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=X[:, [0,1,3,4,5]]\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Variable that corresponds to x2 above has to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   278.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 31 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>1.68e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:49:05</td>     <th>  Log-Likelihood:    </th> <td> -526.81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1062.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1069.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.507e+04</td> <td> 6145.947</td> <td>    8.960</td> <td> 0.000</td> <td> 4.27e+04</td> <td> 6.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> -573.7029</td> <td> 2838.043</td> <td>   -0.202</td> <td> 0.841</td> <td>-6286.386</td> <td> 5138.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.8624</td> <td>    0.030</td> <td>   28.282</td> <td> 0.000</td> <td>    0.801</td> <td>    0.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0530</td> <td>    0.050</td> <td>   -1.063</td> <td> 0.294</td> <td>   -0.154</td> <td>    0.047</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.902</td> <th>  Durbin-Watson:     </th> <td>   1.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.964</td> <th>  Prob(JB):          </th> <td>2.48e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.543</td> <th>  Cond. No.          </th> <td>6.74e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.74e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.948\n",
       "Model:                            OLS   Adj. R-squared:                  0.944\n",
       "Method:                 Least Squares   F-statistic:                     278.7\n",
       "Date:                Sun, 31 Mar 2019   Prob (F-statistic):           1.68e-29\n",
       "Time:                        16:49:05   Log-Likelihood:                -526.81\n",
       "No. Observations:                  50   AIC:                             1062.\n",
       "Df Residuals:                      46   BIC:                             1069.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.507e+04   6145.947      8.960      0.000    4.27e+04    6.74e+04\n",
       "x1          -573.7029   2838.043     -0.202      0.841   -6286.386    5138.981\n",
       "x2             0.8624      0.030     28.282      0.000       0.801       0.924\n",
       "x3            -0.0530      0.050     -1.063      0.294      -0.154       0.047\n",
       "==============================================================================\n",
       "Omnibus:                       14.902   Durbin-Watson:                   1.199\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.212\n",
       "Skew:                          -0.964   Prob(JB):                     2.48e-05\n",
       "Kurtosis:                       5.543   Cond. No.                     6.74e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.74e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=X[:, [0,3,4,5]]\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now the Variable that corresponds to x1 above has to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 31 Mar 2019</td> <th>  Prob (F-statistic):</th>  <td> 0.372</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:49:22</td>     <th>  Log-Likelihood:    </th> <td> -599.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1205.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1211.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 7.613e+04</td> <td> 2.59e+04</td> <td>    2.942</td> <td> 0.005</td> <td> 2.41e+04</td> <td> 1.28e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 2555.2116</td> <td>  1.2e+04</td> <td>    0.212</td> <td> 0.833</td> <td>-2.16e+04</td> <td> 2.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2885</td> <td>    0.205</td> <td>    1.404</td> <td> 0.167</td> <td>   -0.125</td> <td>    0.702</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.119</td> <th>  Durbin-Watson:     </th> <td>   0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.942</td> <th>  Jarque-Bera (JB):  </th> <td>   0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.099</td> <th>  Prob(JB):          </th> <td>   0.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.835</td> <th>  Cond. No.          </th> <td>5.67e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.67e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.041\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     1.010\n",
       "Date:                Sun, 31 Mar 2019   Prob (F-statistic):              0.372\n",
       "Time:                        16:49:22   Log-Likelihood:                -599.60\n",
       "No. Observations:                  50   AIC:                             1205.\n",
       "Df Residuals:                      47   BIC:                             1211.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       7.613e+04   2.59e+04      2.942      0.005    2.41e+04    1.28e+05\n",
       "x1          2555.2116    1.2e+04      0.212      0.833   -2.16e+04    2.68e+04\n",
       "x2             0.2885      0.205      1.404      0.167      -0.125       0.702\n",
       "==============================================================================\n",
       "Omnibus:                        0.119   Durbin-Watson:                   0.097\n",
       "Prob(Omnibus):                  0.942   Jarque-Bera (JB):                0.139\n",
       "Skew:                           0.099   Prob(JB):                        0.933\n",
       "Kurtosis:                       2.835   Cond. No.                     5.67e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.67e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=X[:, [0,3,5]]\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the answers are not matching the course output and my own output on Spyder, I am leaving it as it is. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
